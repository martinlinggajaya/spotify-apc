{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]')\\\n",
    "                    .appName(\"Validation Split\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "\n",
    "# path_to_df = 'gs://thesis_spotify_apc_bucket/df_data'\n",
    "path_to_df = '/Volumes/T5/PROJECTS/U. THESIS/From GCP/df_data2'\n",
    "\n",
    "df_tracks = spark.read.orc(path_to_df + '/df_tracks.orc')\n",
    "df_playlists = spark.read.orc(path_to_df + '/df_playlists.orc')\n",
    "df_playlists_metadata = spark.read.orc(path_to_df + '/df_playlists_metadata.orc')\n",
    "df_challenge_playlists = spark.read.orc(path_to_df + '/df_challenge_playlists.orc')\n",
    "df_challenge_playlists_metadata = spark.read.orc(path_to_df + '/df_challenge_playlists_metadata.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group pid's based on num_tracks\n",
    "\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "num_tracks = df_playlists_metadata.groupBy('num_tracks').agg(collect_list('pid').alias('pid')).orderBy('num_tracks', ascending=True)\n",
    "# num_tracks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of test playlists with different values of num_tracks\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "test_num_tracks_count = df_challenge_playlists_metadata.groupBy('num_tracks').count().sort(desc('count'))\n",
    "# test_num_tracks_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 503 ms, sys: 192 ms, total: 696 ms\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Randomly select pid's to be used as validation\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "validation_playlists = {}\n",
    "\n",
    "for row in test_num_tracks_count.collect():\n",
    "    i = row['num_tracks']\n",
    "    j = row['count']\n",
    "    pid_list = num_tracks.where(col('num_tracks') == i).select('pid').collect()\n",
    "    try:\n",
    "        validation_playlists[i] = np.random.choice(pid_list[0]['pid'], int(3*j), replace=False)\n",
    "    except:\n",
    "        print(len(pid_list[0]['pid']), i, j)\n",
    "        validation_playlists[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 ms, sys: 6.77 ms, total: 23.7 ms\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add and assign pid's to either validation 1 or validation 2\n",
    "\n",
    "val1_playlist = {}\n",
    "val2_playlist = {}\n",
    "test_playlist = {}\n",
    "\n",
    "for i in [0, 1, 5, 10, 25, 100]:\n",
    "    \n",
    "    val1_playlist[i] = []\n",
    "    val2_playlist[i] = []\n",
    "    test_playlist[i] = []\n",
    "    \n",
    "    value_counts = df_challenge_playlists_metadata.where(col('num_samples') == i)\\\n",
    "                                             .groupBy('num_tracks').count()\n",
    "\n",
    "    for row in value_counts.collect():\n",
    "        j = row['num_tracks']\n",
    "        k = row['count']\n",
    "        # print(len(validation_playlists[j]))\n",
    "        val1_playlist[i] += list(validation_playlists[j][:k])\n",
    "        validation_playlists[j] = validation_playlists[j][k:]\n",
    "        val2_playlist[i] += list(validation_playlists[j][:k])\n",
    "        validation_playlists[j] = validation_playlists[j][k:]\n",
    "        test_playlist[i] += list(validation_playlists[j][:k])\n",
    "        validation_playlists[j] = validation_playlists[j][k:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Filtering\n",
    "Validation logic:\n",
    "- Each row in df_playlists is checked whether it belongs to val_playlist\\[i\\] AND its position is >= i\n",
    "- df_train is made up of the other rows that do not belong to val1 or val2\n",
    "\n",
    "Implementatin idea:\n",
    "\n",
    "1. df_train = df_playlists\n",
    "\n",
    "2. Filter df_train; remove the rows belong to val1_playlist\\[0\\] and val2_playlist\\[0\\] and place them to df_val1 and df_val2 respectively\n",
    "\n",
    "3. Continue filtering df_train for i \\[1, 5, 10, 25, 100\\] and append them to df_val1 and df_val2\n",
    "\n",
    "4. What's left in df_train is the rest of the rows that are not in val1 or val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 505 ms, sys: 177 ms, total: 682 ms\n",
      "Wall time: 3.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# df_train = df_playlists\n",
    "\n",
    "filter_val1 = [int(x) for x in val1_playlist[0]]\n",
    "filter_val2 = [int(x) for x in val2_playlist[0]]\n",
    "filter_test = [int(x) for x in test_playlist[0]]\n",
    "\n",
    "# Append to df_val1 and df_val2\n",
    "df_val1 = df_playlists.filter(df_playlists['pid'].isin(filter_val1))\n",
    "df_val2 = df_playlists.filter(df_playlists['pid'].isin(filter_val2))\n",
    "df_test_full = df_playlists.filter(df_playlists['pid'].isin(filter_test))\n",
    "df_test_metadata = df_playlists_metadata.select('pid', 'name', 'num_tracks').filter(df_playlists_metadata['pid'].isin(filter_test))\\\n",
    "                                        .withColumn('num_samples', lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.57 s, sys: 1.59 s, total: 6.17 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in [1, 5, 10, 25, 100]:\n",
    "    \n",
    "    filter_val1 = [int(x) for x in val1_playlist[i]]\n",
    "    filter_val2 = [int(x) for x in val2_playlist[i]]\n",
    "    filter_test = [int(x) for x in test_playlist[i]]\n",
    "    \n",
    "    # Filter and append to df_val1 and df_val2\n",
    "    append_to_val1 = df_playlists.filter((df_playlists['pid'].isin(filter_val1)) & (df_playlists['pos'] >= i))\n",
    "    df_val1 = df_val1.union(append_to_val1)\n",
    "    append_to_val2 = df_playlists.filter((df_playlists['pid'].isin(filter_val2)) & (df_playlists['pos'] >= i))\n",
    "    df_val2 = df_val2.union(append_to_val2)\n",
    "    append_to_test = df_playlists.filter(df_playlists['pid'].isin(filter_test))\n",
    "    df_test_full = df_test_full.union(append_to_test)\n",
    "    append_to_test_metadata = df_playlists_metadata.select('pid', 'name', 'num_tracks').filter(df_playlists_metadata['pid'].isin(filter_test))\\\n",
    "                                                   .withColumn('num_samples', lit(i))\n",
    "    df_test_metadata = df_test_metadata.union(append_to_test_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 1.46 ms, total: 3.74 ms\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Remove rows that are in validation\n",
    "\n",
    "df_train = df_playlists.join(df_val1, ['pid', 'tid', 'pos'], 'leftanti')\\\n",
    "                       .join(df_val2, ['pid', 'tid', 'pos'], 'leftanti')\\\n",
    "                       .join(df_test_full, ['pid', 'tid', 'pos'], 'leftanti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val1_pids = np.hstack([val1_playlist[i] for i in val1_playlist])\n",
    "val2_pids = np.hstack([val2_playlist[i] for i in val2_playlist])\n",
    "test_pids = np.hstack([test_playlist[i] for i in test_playlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(len(val1_pids), len(val2_pids), len(test_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataframes for testing\n",
    "\n",
    "temp_test = df_test_full.join(df_test_metadata.select('pid', 'num_samples'), ['pid'], 'left')\n",
    "\n",
    "# For playlists with num_samples = 1, take only the first 1 tracks\n",
    "df_test_playlists = temp_test.filter((col('num_samples') == 1) & (col('pos') < 1))\n",
    "\n",
    "# For playlists with num_samples = 5, take only the first 5 tracks\n",
    "df_test_playlists = df_test_playlists.union(temp_test.filter((col('num_samples') == 5) & (col('pos') < 5)))\n",
    "\n",
    "# For playlists with num_samples = 10, take only the first 10 tracks\n",
    "df_test_playlists = df_test_playlists.union(temp_test.filter((col('num_samples') == 10) & (col('pos') < 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For playlists with num_samples = 10, 25, and 100, half of them have their tracks picked at random and the other half picked in order\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# random_pids = {}\n",
    "# ordered_pids = {}\n",
    "\n",
    "for i in [25, 100]:\n",
    "    random_pids = [int(x) for x in np.random.choice(test_playlist[i], 1000, replace=False)]\n",
    "    ordered_pids = [int(x) for x in list(set(test_playlist[i]) - set(random_pids))]\n",
    "    \n",
    "    df_test_playlists = df_test_playlists.union(temp_test.filter((col('pid').isin(ordered_pids)) & (col('pos') < i)))\n",
    "    chosen_random = temp_test.filter(col('pid').isin(random_pids))\n",
    "    pd_chosen_random = chosen_random.toPandas().groupby('pid').sample(n=i, random_state=1)\n",
    "    chosen_random = spark.createDataFrame(pd_chosen_random)\n",
    "    df_test_playlists = df_test_playlists.union(chosen_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat df_train with df_test_playlists\n",
    "\n",
    "df_train_final = df_train.union(df_test_playlists.select(['pid', 'tid', 'pos']))  # select to make sure the order of columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|               pid|              tid|               pos|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|          11210327|         11210327|          11210327|\n",
      "|   mean|170987.08450690153|518038.5394889016|49.463758015265746|\n",
      "| stddev| 72343.57508834456|113800.6593490638|44.754728705095445|\n",
      "|    min|                 0|                0|                 0|\n",
      "|    max|            278999|           995381|               249|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_final.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as ORC files\n",
    "\n",
    "# path_to_write = 'gs://thesis_spotify_apc_bucket/df_data'\n",
    "path_to_write = '/Volumes/T5/PROJECTS/U. THESIS/Dataset/new_df_data'\n",
    "\n",
    "df_train_final.write.mode('overwrite').orc(path_to_write + '/df_train.orc')\n",
    "df_val1.write.mode('overwrite').format('orc').save(path_to_write + '/df_val1.orc')\n",
    "df_val2.write.mode('overwrite').format('orc').save(path_to_write + '/df_val2.orc')\n",
    "df_test_full.write.mode('overwrite').format('orc').save(path_to_write + '/df_test_full.orc')\n",
    "df_test_playlists.write.mode('overwrite').format('orc').save(path_to_write + '/df_test_playlists.orc')\n",
    "df_test_metadata.write.mode('overwrite').format('orc').save(path_to_write + '/df_test_metadata.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to_save/test_pids.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# !mkdir to_save\n",
    "\n",
    "joblib.dump(val1_pids, path_to_write + '/val1_pids.pkl')\n",
    "joblib.dump(val2_pids, path_to_write + '/val2_pids.pkl')\n",
    "joblib.dump(test_pids, path_to_write + '/test_pids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://to_save/test_pids.pkl [Content-Type=application/octet-stream]...\n",
      "Copying file://to_save/val1_pids.pkl [Content-Type=application/octet-stream]... \n",
      "Copying file://to_save/val2_pids.pkl [Content-Type=application/octet-stream]... \n",
      "/ [3 files][234.9 KiB/234.9 KiB]                                                \n",
      "Operation completed over 3 objects/234.9 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# Upload file to GCS\n",
    "\n",
    "# ! gsutil cp to_save/* gs://thesis_spotify_apc_bucket/df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as HDF files (CHECKING)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train = df_train_final.toPandas()\n",
    "val1 = df_val1.toPandas()\n",
    "val2 = df_val2.toPandas()\n",
    "\n",
    "train.to_hdf(path_to_write + '/df_train.hdf', key='abc')\n",
    "val1.to_hdf(path_to_write + '/df_val1.hdf', key='abc')\n",
    "val2.to_hdf(path_to_write + '/df_val2.hdf', key='abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://df_data/df_train.hdf [Content-Type=application/x-hdf]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][342.1 MiB/342.1 MiB]                                                \n",
      "Operation completed over 1 objects/342.1 MiB.                                    \n",
      "Copying file://df_data/df_val1.hdf [Content-Type=application/x-hdf]...\n",
      "/ [1 files][ 21.3 MiB/ 21.3 MiB]                                                \n",
      "Operation completed over 1 objects/21.3 MiB.                                     \n",
      "Copying file://df_data/df_val2.hdf [Content-Type=application/x-hdf]...\n",
      "/ [1 files][ 21.3 MiB/ 21.3 MiB]                                                \n",
      "Operation completed over 1 objects/21.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# ! gsutil cp df_data/df_train.hdf gs://thesis_spotify_apc_bucket/df_data/df_train.hdf\n",
    "# ! gsutil cp df_data/df_val1.hdf gs://thesis_spotify_apc_bucket/df_data/df_val1.hdf\n",
    "# ! gsutil cp df_data/df_val2.hdf gs://thesis_spotify_apc_bucket/df_data/df_val2.hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ketinggalan save\n",
    "\n",
    "# path_to_write = 'gs://thesis_spotify_apc_bucket/df_data'\n",
    "\n",
    "# df_test_full = spark.read.orc(path_to_write + '/df_test_full.orc')\n",
    "# df_test_playlists = spark.read.orc(path_to_write + '/df_test_playlists.orc')\n",
    "# df_test_metadata = spark.read.orc(path_to_write + '/df_test_metadata.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# test_full = df_test_full.toPandas()\n",
    "# test_playlists = df_test_playlists.toPandas()\n",
    "# test_metadata = df_test_metadata.toPandas()\n",
    "\n",
    "# test_full.to_hdf('df_data/df_test_full.hdf', key='abc')\n",
    "# test_playlists.to_hdf('df_data/df_test_playlists.hdf', key='abc')\n",
    "# test_metadata.to_hdf('df_data/df_test_metadata.hdf', key='abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://df_data/df_test_full.hdf [Content-Type=application/x-hdf]...\n",
      "/ [1 files][ 29.9 MiB/ 29.9 MiB]                                                \n",
      "Operation completed over 1 objects/29.9 MiB.                                     \n",
      "Copying file://df_data/df_test_playlists.hdf [Content-Type=application/x-hdf]...\n",
      "/ [1 files][  9.7 MiB/  9.7 MiB]                                                \n",
      "Operation completed over 1 objects/9.7 MiB.                                      \n",
      "Copying file://df_data/df_test_metadata.hdf [Content-Type=application/x-hdf]...\n",
      "/ [1 files][  1.4 MiB/  1.4 MiB]                                                \n",
      "Operation completed over 1 objects/1.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# ! gsutil cp df_data/df_test_full.hdf gs://thesis_spotify_apc_bucket/df_data/df_test_full.hdf\n",
    "# ! gsutil cp df_data/df_test_playlists.hdf gs://thesis_spotify_apc_bucket/df_data/df_test_playlists.hdf\n",
    "# ! gsutil cp df_data/df_test_metadata.hdf gs://thesis_spotify_apc_bucket/df_data/df_test_metadata.hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
