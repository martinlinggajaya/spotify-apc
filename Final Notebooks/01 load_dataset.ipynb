{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'data'\n",
    "bucket_name = 'thesis_spotify_apc_bucket'\n",
    "path_to_write = 'gs://thesis_spotify_apc_bucket/df_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_cols = ['pid', 'name', 'modified_at', 'num_artists', 'num_albums', \n",
    "                 'num_tracks', 'num_followers', 'num_edits', 'duration_ms', 'collaborative']\n",
    "\n",
    "track_cols = ['album_name', 'album_uri', 'artist_name', 'artist_uri', \n",
    "              'duration_ms', 'track_name', 'track_uri']\n",
    "\n",
    "playlist_test_cols = ['name', 'num_holdouts', 'num_samples', 'num_tracks', 'pid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"Load Dataset\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train playlist metadata\n",
    "data_playlists = []\n",
    "# Track metada\n",
    "data_tracks = []\n",
    "# Train playlist ID with track URI and position\n",
    "playlists = []\n",
    "# Track URIs\n",
    "tracks = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "CPU times: user 6min 35s, sys: 12.7 s, total: 6min 48s\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load train data to lists\n",
    "\n",
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "train_path = path_to_dataset + '/mpd/data/'\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "limit = 200\n",
    "\n",
    "# for filename in filenames:\n",
    "for blob in client.list_blobs(bucket_name, prefix=train_path):\n",
    "    file_count += 1\n",
    "    # Read file\n",
    "    mpd_slice = json.loads(blob.download_as_string(client=None))\n",
    "    for playlist in mpd_slice['playlists']:\n",
    "        data_playlists.append([playlist[col] for col in playlist_cols])\n",
    "        for track in playlist['tracks']:\n",
    "            playlists.append([playlist['pid'], track['track_uri'], track['pos']])\n",
    "            if track['track_uri'] not in tracks:\n",
    "                data_tracks.append([track[col] for col in track_cols])\n",
    "                tracks.add(track['track_uri'])\n",
    "    # For every 100 files, save and reset\n",
    "    if file_count % 100 == 0:\n",
    "        print(file_count)\n",
    "        # Save\n",
    "        temp_playlists_metadata = spark.createDataFrame(data=data_playlists, schema=playlist_cols)\n",
    "        temp_tracks = spark.createDataFrame(data=data_tracks, schema=track_cols)\n",
    "        temp_playlists = spark.createDataFrame(data=playlists, schema=['pid', 'track_uri', 'pos'])\n",
    "        part_count = str(int(file_count / 100))\n",
    "        temp_playlists_metadata.write.mode('overwrite').orc(path_to_write + '/temp_playlists_metadata_' + part_count + '.orc')\n",
    "        temp_tracks.write.mode('overwrite').orc(path_to_write + '/temp_tracks_' + part_count + '.orc')\n",
    "        temp_playlists.write.mode('overwrite').orc(path_to_write + '/temp_playlists_' + part_count + '.orc')\n",
    "        # Reset\n",
    "        data_playlists = []\n",
    "        playlists = []\n",
    "        data_tracks = []\n",
    "    if file_count == limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test playlist metadata\n",
    "data_playlists_test = []\n",
    "# Test playlist ID with track URI and position\n",
    "playlists_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 705 ms, total: 3.15 s\n",
      "Wall time: 7.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load test data to lists\n",
    "\n",
    "test_path = path_to_dataset + '/challenge_set.json'\n",
    "\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(test_path)\n",
    "mpd_slice = json.loads(blob.download_as_string(client=None))\n",
    "\n",
    "for playlist in mpd_slice['playlists']:\n",
    "    data_playlists_test.append([playlist.get(col, '') for col in playlist_test_cols])\n",
    "    for track in playlist['tracks']:\n",
    "        playlists_test.append([playlist['pid'], track['track_uri'], track['pos']])\n",
    "        if track['track_uri'] not in tracks:\n",
    "            data_tracks.append([track[col] for col in track_cols])\n",
    "            tracks.add(track['track_uri'])\n",
    "\n",
    "# Save data_tracks into new temp_tracks\n",
    "if data_tracks is not None:\n",
    "    temp_tracks = spark.createDataFrame(data=data_tracks, schema=track_cols)\n",
    "    temp_tracks.write.mode('overwrite').orc(path_to_write + '/temp_tracks_11.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.4 ms, sys: 298 µs, total: 44.7 ms\n",
      "Wall time: 8.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Turn lists into dataframes\n",
    "# Playlist and track metadata dataframes\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_playlists_metadata = spark.read.orc(path_to_write + '/temp_playlists_metadata_*.orc')\n",
    "df_playlists_metadata = df_playlists_metadata.withColumn('collaborative', \n",
    "                                                         when(col('collaborative') == 'true', 1).when(col('collaborative') == 'false', 0))\n",
    "\n",
    "df_tracks = spark.read.orc(path_to_write + '/temp_tracks_*.orc')\n",
    "\n",
    "# Generate tid from index\n",
    "tracks_rdd = df_tracks.rdd.zipWithIndex()\n",
    "df_tracks = tracks_rdd.toDF()\n",
    "# Break list in column _1 to the original schema\n",
    "for col in track_cols:\n",
    "    df_tracks = df_tracks.withColumn(col, df_tracks['_1'].getItem(col))\n",
    "\n",
    "df_tracks = df_tracks.withColumnRenamed('_2', 'tid') \\\n",
    "                     .drop('_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 ms, sys: 3.93 ms, total: 5.93 ms\n",
      "Wall time: 567 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Turn lists into dataframes\n",
    "# Train playlist dataframes\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_playlists = spark.read.orc(path_to_write + '/temp_playlists_*.orc')\n",
    "\n",
    "# Remove rows in df_playlists with null values\n",
    "df_playlists = df_playlists.filter(col('pos').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING - Check for nan and null values in df_train\n",
    "\n",
    "# from pyspark.sql.functions import when, isnan, count, col\n",
    "\n",
    "# df_playlists.select([count(when(isnan(c), c)).alias(c) for c in df_playlists.columns]).show()\n",
    "# df_playlists.select([count(when(col(c).isNull(), c)).alias(c) for c in df_playlists.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.47 ms, sys: 118 µs, total: 7.59 ms\n",
      "Wall time: 47.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert track_uri into tid in df_playlists\n",
    "\n",
    "df_playlists = df_playlists.join(df_tracks.select('track_uri', 'tid'), ['track_uri'], 'left') \\\n",
    "                           .drop('track_uri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.24 s, sys: 15.5 ms, total: 5.26 s\n",
      "Wall time: 5.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Turn lists into dataframes\n",
    "# Test playlist and metadata dataframes\n",
    "\n",
    "df_challenge_playlists_metadata = spark.createDataFrame(data=data_playlists_test, schema=playlist_test_cols)\n",
    "\n",
    "df_challenge_playlists = spark.createDataFrame(data=playlists_test, schema=['pid', 'track_uri', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.52 ms, sys: 0 ns, total: 7.52 ms\n",
      "Wall time: 39.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert track_uri to tid in df_challenge_playlists\n",
    "\n",
    "df_challenge_playlists = df_challenge_playlists.join(df_tracks.select('track_uri', 'tid'), ['track_uri'], 'left') \\\n",
    "                                     .drop('track_uri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 ms, sys: 150 µs, total: 18.7 ms\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save dataframes as ORC files\n",
    "\n",
    "df_tracks.write.mode('overwrite').orc(path_to_write + '/df_tracks.orc')\n",
    "df_playlists_metadata.write.mode('overwrite').orc(path_to_write + '/df_playlists_metadata.orc')\n",
    "df_playlists.write.mode('overwrite').orc(path_to_write + '/df_playlists.orc')\n",
    "df_challenge_playlists.write.mode('overwrite').orc(path_to_write + '/df_challenge_playlists.orc')\n",
    "df_challenge_playlists_metadata.write.mode('overwrite').orc(path_to_write + '/df_challenge_playlists_metadata.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as HDF files\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train = df_train.toPandas()\n",
    "# val1 = df_val1.toPandas()\n",
    "# val2 = df_val2.toPandas()\n",
    "\n",
    "# train.to_hdf(path_to_write + '/train.hdf', key='abc')\n",
    "# val1.to_hdf(path_to_write + '/val1.hdf', key='abc')\n",
    "# val2.to_hdf(path_to_write + '/val2.hdf', key='abc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
